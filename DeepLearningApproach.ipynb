{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub\n",
    "import tensorflow_text\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import openpyxl\n",
    "import warnings\n",
    "import random\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "\n",
    "import matplotlib.font_manager\n",
    "warnings.filterwarnings('ignore')  # Filter Errors\n",
    "plt.rcParams['font.family'] = 'Times New Roman'  # Set plt shows font to Times New Roman\n",
    "plt.rcParams['axes.grid'] = True  # Ensure line graphs display on graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    \"\"\"\n",
    "    Removes stopwords, words shorter than 3 characters, all links (href), emojis, & punctuation.\n",
    "    :type text: string\n",
    "    \"\"\"\n",
    "    additional_stopwords = set(['br', 'the', 'i', 'me', 'my', 'myself', \n",
    "                                'we', 'our', 'ours', 'ourselves', 'you', \n",
    "                                \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', \n",
    "                                'yours', 'yourself', 'yourselves', 'he', 'him', \n",
    "                                'his', 'himself', 'she', \"she's\", 'her', 'hers',\n",
    "                                'herself', 'it', \"it's\", 'its', 'itself', 'they',\n",
    "                                'them', 'their', 'theirs', 'themselves', 'what',\n",
    "                                'which', 'who', 'whom', 'this', 'that', \"that'll\",\n",
    "                                'these', 'those', 'am', 'is', 'are', 'was', 'were',\n",
    "                                'be', 'been', 'being', 'have', 'has', 'had', 'having',\n",
    "                                'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and',\n",
    "                                'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of',\n",
    "                                'at', 'by', 'for', 'with', 'about', 'against', 'between',\n",
    "                                'into', 'through', 'during', 'before', 'after', 'above', \n",
    "                                'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off',\n",
    "                                'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "                                'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both',\n",
    "                                'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only',\n",
    "                                'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can',\n",
    "                                'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now',\n",
    "                                'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", \n",
    "                                'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \n",
    "                                \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma',\n",
    "                                'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \n",
    "                                \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\",\n",
    "                                'won', \"won't\", 'wouldn', \"wouldn't\"])\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    text = text[2:-1]  # Remove the leading b, it looks like it's encoded but it's not\n",
    "    text = text.replace('&amp;', 'and')\n",
    "    text = text.replace(r\"\\'s\", r\"'s'\")\n",
    "    text = re.sub(r'[0-9]', ' ', text) # Match all digits in the string and replace them by empty string\n",
    "    text = ' '.join(filter(lambda x:x[0]!=r'//', text.split()))\n",
    "    text = ' '.join(re.sub(r'(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)',' ', text).split())  # Remove hashtags #Derek, would be '' & https: links\n",
    "    \n",
    "    text = re.sub(emoji_pattern, '', text)  # Remove all emoji patterns\n",
    "    text = re.sub(\"<a.+?>\", ' ', text)  # Remove url, but keep the label\n",
    "    \n",
    "    html_tag_cleaner = re.compile('<.*?>')  # Remove any html div\n",
    "    text = re.sub(html_tag_cleaner, '', text)  # Execute html removal\n",
    "    \n",
    "    text = re.sub(r'[^A-Za-z0-9 ]+', ' ', text)  # Remove all special characters\n",
    "    text = re.sub(r'\\b\\w{1,3}\\b', ' ', text)  # Remove all words less than or equal to 3 characters\n",
    "    text = text.lower()  # Lower case text\n",
    "    text = remove_stopwords(text)  # Removes stop words\n",
    "    # Remove contractions or other custom stopwords\n",
    "    text = ' '.join(e.lower() for e in text.split() if e.lower() not in additional_stopwords)\n",
    "    \n",
    "    text = re.sub(r\"won't\", \"will not\", text)  # Replace these words\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)  # Replace these words\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)  # Replace these words\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)  # Replace these words\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)  # Replace these words\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)  # Replace these words\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)  # Replace these words\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)  # Replace these words\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)  # Replace these words\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)  # Replace these words\n",
    "    \n",
    "    text = re.sub(' +', ' ', text)  # Remove double whitespace\n",
    "    text = text.strip()  # Remove leading & trailing whitespace\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tags the words in the text\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return(nltk.corpus.wordnet.ADJ)\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return(nltk.corpus.wordnet.VERB)\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return(nltk.corpus.wordnet.NOUN)\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return(nltk.corpus.wordnet.ADV)\n",
    "    else:\n",
    "        return(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# Lemmatizes the words in texts and returns the cleaned and lemmatized text\n",
    "def lemmatize_text(text):\n",
    "    # Tokenize the text and find the POS tag for each token\n",
    "    text = text_cleaner(text)\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(text))  \n",
    "    # Tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_text = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None: # If there is no available tag, append the token as is\n",
    "            lemmatized_text.append(word)\n",
    "        else:  # Else use the tag to lemmatize the token\n",
    "            lemmatized_text.append(lemmatizer.lemmatize(word, tag))\n",
    "    return(' '.join(lemmatized_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "<a class='anchor' id='model-creation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the raw dataset\n",
    "training_tweet_df = pd.read_csv(r'congressional_tweet_training_data.csv')\n",
    "test_tweet_df = pd.read_csv(r'congressional_tweet_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_party_dict = {\n",
    "    0: 'D',\n",
    "    1: 'R'\n",
    "}\n",
    "\n",
    "party_to_int_dict = {\n",
    "    'D': 0,\n",
    "    'R': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training_tweet_df['full_text']\n",
    "y_train = training_tweet_df['party_id'].map(party_to_int_dict)\n",
    "y_train = y_train.values\n",
    "\n",
    "X_test = test_tweet_df['full_text']\n",
    "y_test = test_tweet_df['party'].map(party_to_int_dict)\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['full_text_clean'] = X_train.apply(lambda x: lemmatize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocess = tensorflow_hub.KerasLayer(r'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n",
    "bert_encoder = tensorflow_hub.KerasLayer(r'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method KerasLayer.call of <tensorflow_hub.keras_layer.KerasLayer object at 0x2b9cc105c310>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method KerasLayer.call of <tensorflow_hub.keras_layer.KerasLayer object at 0x2b9cc105c310>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method KerasLayer.call of <tensorflow_hub.keras_layer.KerasLayer object at 0x2b9cc105c310>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "\n",
    "preprocessed_text = bert_preprocess(text_input)\n",
    "outputs = bert_encoder(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = tf.keras.layers.Dropout(0.1, name='dropout')(outputs['pooled_output'])\n",
    "l = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(l)\n",
    "\n",
    "model = tf.keras.Model(inputs=[text_input], outputs = [l])\n",
    "\n",
    "metrics = [\n",
    "    tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall')\n",
    "]\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "18526/18526 [==============================] - 966s 52ms/step - loss: 0.6619 - accuracy: 0.6019 - precision: 0.5739 - recall: 0.4640\n",
      "Epoch 2/30\n",
      "18526/18526 [==============================] - 900s 49ms/step - loss: 0.6452 - accuracy: 0.6228 - precision: 0.5971 - recall: 0.5129\n",
      "Epoch 3/30\n",
      "18526/18526 [==============================] - 910s 49ms/step - loss: 0.6451 - accuracy: 0.6245 - precision: 0.5987 - recall: 0.5169\n",
      "Epoch 4/30\n",
      "18526/18526 [==============================] - 899s 49ms/step - loss: 0.6443 - accuracy: 0.6244 - precision: 0.5984 - recall: 0.5175\n",
      "Epoch 5/30\n",
      "18526/18526 [==============================] - 898s 48ms/step - loss: 0.6448 - accuracy: 0.6249 - precision: 0.5993 - recall: 0.5185\n",
      "Epoch 6/30\n",
      "18526/18526 [==============================] - 898s 48ms/step - loss: 0.6457 - accuracy: 0.6242 - precision: 0.5979 - recall: 0.5150\n",
      "Epoch 7/30\n",
      "18526/18526 [==============================] - 898s 48ms/step - loss: 0.6456 - accuracy: 0.6230 - precision: 0.5973 - recall: 0.5197\n",
      "Epoch 8/30\n",
      "18526/18526 [==============================] - 898s 48ms/step - loss: 0.6447 - accuracy: 0.6244 - precision: 0.5982 - recall: 0.5184\n",
      "Epoch 9/30\n",
      "18526/18526 [==============================] - 898s 48ms/step - loss: 0.6448 - accuracy: 0.6240 - precision: 0.5974 - recall: 0.5150\n",
      "Epoch 10/30\n",
      "18526/18526 [==============================] - 898s 48ms/step - loss: 0.6446 - accuracy: 0.6246 - precision: 0.5986 - recall: 0.5151\n",
      "Epoch 11/30\n",
      "18526/18526 [==============================] - 899s 49ms/step - loss: 0.6447 - accuracy: 0.6240 - precision: 0.5980 - recall: 0.5185\n",
      "Epoch 12/30\n",
      "18526/18526 [==============================] - 898s 48ms/step - loss: 0.6455 - accuracy: 0.6232 - precision: 0.5980 - recall: 0.5198\n",
      "Epoch 13/30\n",
      "18526/18526 [==============================] - 900s 49ms/step - loss: 0.6450 - accuracy: 0.6243 - precision: 0.5997 - recall: 0.5212\n",
      "Epoch 14/30\n",
      "18526/18526 [==============================] - 901s 49ms/step - loss: 0.6456 - accuracy: 0.6241 - precision: 0.5984 - recall: 0.5195\n",
      "Epoch 15/30\n",
      "18526/18526 [==============================] - 898s 48ms/step - loss: 0.6444 - accuracy: 0.6240 - precision: 0.5985 - recall: 0.5208\n",
      "Epoch 16/30\n",
      "16782/18526 [==========================>...] - ETA: 1:24 - loss: 0.6443 - accuracy: 0.6246 - precision: 0.5990 - recall: 0.5193"
     ]
    }
   ],
   "source": [
    "model.fit(X_train['full_text_clean'].to_numpy(), y_train, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet_party(test_x_dataframe, trained_model):\n",
    "    \"\"\"\n",
    "    Pipeline for preprocessing and prediciton.\n",
    "    \"\"\"\n",
    "#     Clean the Summary Text column\n",
    "    test_x_dataframe['full_text_clean'] = test_x_dataframe.apply(lambda x: lemmatize_text(x))\n",
    "    predictions = trained_model.predict(test_x_dataframe.to_numpy())  # Predict\n",
    "    predictions = predictions.flatten()\n",
    "    predictions = np.round(predictions)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_tweet_party(X_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "<a class='anchor' id='prediction'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = test_tweet_df.copy()\n",
    "submission['party'] = predictions\n",
    "submission['party'] = submission['party'].map(int_to_party_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[['Id', 'party']].to_csv('output.csv', index=False)  # Export these features without the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaulation\n",
    "<a class='anchor' id='evaulation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_true=y_test, y_pred=predictions)  # Calculate the confusion matrix\n",
    "\n",
    "plt.figure(figsize = (10,7))  # Initialize plot figure\n",
    "ax = sns.heatmap(conf_matrix, # Heat map the confusion matrix\n",
    "                 annot=True,   # Plot the counts on each block\n",
    "                 cmap='gist_heat_r',  # Reverse the heat map so it is more bright\n",
    "                )\n",
    "\n",
    "ax.set(xlabel='Predicted', ylabel='Ground Truth', title='Party Confusion Matrix')\n",
    "print(classification_report(y_true=y_test, y_pred=predictions))  # Print the classification report\n",
    "plt.show()  # Show the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (tensorflow 2.4.1)",
   "language": "python",
   "name": "tensorflow-2.4.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
